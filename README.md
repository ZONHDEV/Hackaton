
1. **Question** ‚Üí Saisie de l'utilisateur
2. **Encodage** ‚Üí Embeddings avec MiniLM multilingue
3. **Recherche** ‚Üí Similarit√© vectorielle avec FAISS
4. **Contexte** ‚Üí Extraction des documents pertinents
5. **G√©n√©ration** ‚Üí R√©ponse contextuelle avec DialoGPT-medium
6. **R√©ponse** ‚Üí Retour avec sources cit√©es

## üõ†Ô∏è Technologies Open Source Utilis√©es

### Composants Principaux

| Composant | Technologie | Sp√©cifications | Licence |
|-----------|-------------|----------------|---------|
| **Embeddings** | `paraphrase-multilingual-MiniLM-L12-v2` | 384 dimensions, multilingue, l√©ger | Apache 2.0 |
| **Vector DB** | **FAISS** (Facebook AI Similarity Search) | Index FlatIP, recherche rapide | MIT |
| **LLM** | **DialoGPT-medium** | 345M param√®tres, optimis√© dialogue | MIT |
| **Framework ML** | **PyTorch** + **Transformers** | Inf√©rence CPU, gestion m√©moire | BSD/Apache 2.0 |
| **Embeddings** | **Sentence-Transformers** | Encodage par batch, normalisation | Apache 2.0 |

### Caract√©ristiques Techniques

- **üñ•Ô∏è Compatible CPU** : Fonctionne sur machines 8GB RAM
- **‚ö° Optimisations m√©moire** : 
  - Encodage par batch (16-32 documents)
  - Limitation contexte (400-600 caract√®res)
  - G√©n√©ration contr√¥l√©e (100-150 tokens)
- **üåç Multilingue** : Support fran√ßais/langues locales
- **üíæ Local uniquement** : Aucune connexion internet requise

## üìä Performance et Optimisations

### Gestion M√©moire
```python
# Encodage optimis√©
embeddings = model.encode(texts, batch_size=16, normalize_embeddings=True)

# G√©n√©ration contr√¥l√©e
outputs = model.generate(
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True
)
